{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 19: NLP Basics: Tokenization & Embeddings\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal: Tokenize text and use embeddings.\n\n**Deliverable:** Tokenize `text_sample.txt` and build simple word counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nltk.tokenize import word_tokenize\ntext = open('../../datasets/text_sample.txt').read()\nprint(text)\ntokens = word_tokenize(text.lower())\nprint('Tokens:', tokens)\n# TODO: remove stopwords and compute frequency"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercises (TODO)\n- Build a vocabulary and map tokens to indices."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}